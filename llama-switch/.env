# llama.cpp 二进制文件路径
LLAMA_SERVER_PATH=E:/Downloads/llama-b5293-bin-win-cuda-cu12.4-x64/llama-server.exe
LLAMA_BENCH_PATH=E:/Downloads/llama-b5293-bin-win-cuda-cu12.4-x64/llama-bench.exe

# 模型目录
MODELS_DIR=E:/develop/Models/DeepSeek-R1-Distill-Qwen-7B-GGUF

# API服务器配置
SERVER_HOST=127.0.0.1
SERVER_PORT=8080
SERVER_TIMEOUT=600

# 默认模型配置
DEFAULT_THREADS=8
DEFAULT_CTX_SIZE=4096
DEFAULT_BATCH_SIZE=512
DEFAULT_UBATCH_SIZE=512

# GPU配置
DEFAULT_GPU_LAYERS=99
DEFAULT_SPLIT_MODE=layer
DEFAULT_MAIN_GPU=0
ENABLE_FLASH_ATTN=true

# 缓存配置
DEFAULT_CACHE_TYPE_K=f16
DEFAULT_CACHE_TYPE_V=f16

# 内存管理
ENABLE_MLOCK=false
ENABLE_MMAP=true
NUMA_STRATEGY=

# 日志配置
LOG_LEVEL=info
LOG_FILE=
ENABLE_CONSOLE_LOG=true

# 安全配置
API_KEY=
SSL_KEY_FILE=
SSL_CERT_FILE=